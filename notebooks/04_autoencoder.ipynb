{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "14fcffbc",
            "metadata": {},
            "source": [
                "# 04. Convolutional Autoencoder (Unsupervised)\n",
                "\n",
                "## Introduction\n",
                "This notebook implements a Convolutional Autoencoder (CAE) for unsupervised anomaly detection.\n",
                "The model is trained ONLY on normal images to learn to reconstruct them.\n",
                "Anomalies are detected by high reconstruction error.\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfa73889",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, models\n",
                "\n",
                "tf.random.set_seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb76c98b",
            "metadata": {},
            "source": [
                "## 1. Data Loading\n",
                "For the Autoencoder, we train ONLY on the 'train/good' folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "beee0df6",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def load_data(processed_dir, target_category=None, img_size=(256, 256), batch_size=32):\n",
                "    \"\"\"\n",
                "    Loads ALL data from processed_dir (augmented).\n",
                "    Train: 80% of 'good' samples.\n",
                "    Val: 10% of 'good' samples (optional, but good for monitoring).\n",
                "    Test: 10% of 'good' samples + ALL 'anomaly' samples.\n",
                "    \"\"\"\n",
                "    logger.info(f\"Loading augmented data for category: {target_category}...\")\n",
                "    \n",
                "    if not os.path.exists(processed_dir):\n",
                "        raise ValueError(f\"Processed directory {processed_dir} not found! Please run augmentation first.\")\n",
                "        \n",
                "    # 1. Load All Augmented Data\n",
                "    all_files = []\n",
                "    logger.info(f\"Scanning processed data in {processed_dir}...\")\n",
                "    search_path = os.path.join(processed_dir, \"*\", \"*.png\")\n",
                "    \n",
                "    for filepath in glob.glob(search_path):\n",
                "        folder_name = os.path.basename(os.path.dirname(filepath))\n",
                "        if target_category and not folder_name.startswith(target_category):\n",
                "            continue\n",
                "        \n",
                "        # Determine if good or anomaly\n",
                "        # folder_name ends with '_good' for normal\n",
                "        is_good = folder_name.endswith('_good')\n",
                "        \n",
                "        all_files.append({\n",
                "            'filepath': filepath,\n",
                "            'label_str': folder_name,\n",
                "            'is_good': is_good\n",
                "        })\n",
                "            \n",
                "    df = pd.DataFrame(all_files)\n",
                "    if df.empty:\n",
                "        raise ValueError(\"No data found in processed directory!\")\n",
                "        \n",
                "    # 2. Split Data\n",
                "    good_df = df[df['is_good'] == True]\n",
                "    anomaly_df = df[df['is_good'] == False]\n",
                "    \n",
                "    if good_df.empty:\n",
                "        raise ValueError(\"No normal (good) data found!\")\n",
                "        \n",
                "    # Split 'good' into 80/10/10\n",
                "    # Train (80%)\n",
                "    train_good, temp_good = train_test_split(good_df, train_size=0.8, random_state=42)\n",
                "    \n",
                "    # Val (10%) and Test (10%)\n",
                "    val_good, test_good = train_test_split(temp_good, test_size=0.5, random_state=42)\n",
                "    \n",
                "    # Test set includes test_good and ALL anomalies\n",
                "    test_df = pd.concat([test_good, anomaly_df], ignore_index=True)\n",
                "    \n",
                "    # Create labels for Test: 0 for good, 1 for anomaly\n",
                "    test_df['label'] = test_df['is_good'].apply(lambda x: 0 if x else 1)\n",
                "    \n",
                "    logger.info(f\"Train (Normal): {len(train_good)}\")\n",
                "    logger.info(f\"Test (Normal+Anomaly): {len(test_df)} (Normal: {len(test_good)}, Anomaly: {len(anomaly_df)})\")\n",
                "    \n",
                "    # 3. Create Datasets\n",
                "    def process_path(filepath):\n",
                "        img = tf.io.read_file(filepath)\n",
                "        img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
                "        img = tf.image.resize(img, img_size)\n",
                "        img = tf.cast(img, tf.float32) / 255.0\n",
                "        return img\n",
                "\n",
                "    def process_path_label(filepath, label):\n",
                "        img = process_path(filepath)\n",
                "        return img, label\n",
                "\n",
                "    # Train DS: (x, x) for Autoencoder\n",
                "    train_ds = tf.data.Dataset.from_tensor_slices(train_good['filepath'].values)\n",
                "    train_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "    train_ds = train_ds.shuffle(1000).map(lambda x: (x, x))\n",
                "    train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
                "    \n",
                "    # Test DS: (x, label) for evaluation\n",
                "    test_ds = tf.data.Dataset.from_tensor_slices((test_df['filepath'].values, test_df['label'].values))\n",
                "    test_ds = test_ds.map(process_path_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "    test_ds = test_ds.batch(1) # Batch 1 for individual prediction\n",
                "    \n",
                "    return train_ds, test_ds\n",
                "\n",
                "# --- Configuration ---\n",
                "IMG_SIZE = (256, 256)\n",
                "BATCH_SIZE = 16\n",
                "PROCESSED_DIR = \"../data/processed/augmented\"\n",
                "TARGET_CATEGORY = 'bottle' # Train only on this category\n",
                "\n",
                "# --- Execution ---\n",
                "train_ds, test_ds = load_data(PROCESSED_DIR, TARGET_CATEGORY, IMG_SIZE, BATCH_SIZE)\n",
                "print(\"Data loaded successfully.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "931cff80",
            "metadata": {},
            "source": [
                "## 2. Model Architecture\n",
                "Encoder-Decoder architecture."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c95bdcb6",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_autoencoder(input_shape):\n",
                "    # Encoder\n",
                "    inputs = layers.Input(shape=input_shape)\n",
                "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(inputs)\n",
                "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    \n",
                "    # Latent space\n",
                "    shape_before_flattening = tf.keras.backend.int_shape(x)[1:]\n",
                "    x = layers.Flatten()(x)\n",
                "    latent = layers.Dense(128, name='latent_vector')(x)\n",
                "    \n",
                "    # Decoder\n",
                "    x = layers.Dense(np.prod(shape_before_flattening))(latent)\n",
                "    x = layers.Reshape(shape_before_flattening)(x)\n",
                "    \n",
                "    x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    \n",
                "    outputs = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
                "    \n",
                "    model = models.Model(inputs, outputs, name='autoencoder')\n",
                "    return model\n",
                "\n",
                "autoencoder = create_autoencoder(IMG_SIZE + (3,))\n",
                "autoencoder.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b28e0cfc",
            "metadata": {},
            "source": [
                "## 3. Training\n",
                "Loss function is Mean Squared Error (MSE) between input and output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "978ac196",
            "metadata": {},
            "outputs": [],
            "source": [
                "autoencoder.compile(optimizer='adam', loss='mse')\n",
                "\n",
                "history = autoencoder.fit(\n",
                "    train_ds,\n",
                "    epochs=20,\n",
                "    # In unsupervised setting, we often use a split of train set as validation,\n",
                "    # or just monitor loss.\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "17b6c377",
            "metadata": {},
            "source": [
                "# Create Test Dataset (with labels for evaluation)\n",
                "def process_path_label(filepath, label):\n",
                "    img = tf.io.read_file(filepath)\n",
                "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
                "    img = tf.image.resize(img, IMG_SIZE)\n",
                "    img = tf.cast(img, tf.float32) / 255.0\n",
                "    return img, label\n",
                "\n",
                "def create_test_dataset(dataframe):\n",
                "    filepaths = dataframe['filepath'].values\n",
                "    labels = dataframe['label'].values\n",
                "    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
                "    ds = ds.map(process_path_label, num_parallel_calls=AUTOTUNE)\n",
                "    ds = ds.batch(1) # Batch size 1 for individual prediction\n",
                "    return ds\n",
                "\n",
                "test_ds = create_test_dataset(test_df)\n",
                "\n",
                "def predict_anomaly(model, dataset, threshold=None):\n",
                "    reconstruction_errors = []\n",
                "    labels = []\n",
                "    \n",
                "    for image, label in dataset:\n",
                "        reconstructed = model.predict(image, verbose=0)\n",
                "        loss = np.mean(np.abs(image - reconstructed))\n",
                "        reconstruction_errors.append(loss)\n",
                "        labels.append(label.numpy()[0])\n",
                "        \n",
                "    return np.array(reconstruction_errors), np.array(labels)\n",
                "\n",
                "print(\"Predicting anomalies on test set...\")\n",
                "errors, labels = predict_anomaly(autoencoder, test_ds)\n",
                "\n",
                "# Determine threshold (e.g., 90th percentile of errors)\n",
                "threshold = np.percentile(errors, 90)\n",
                "print(f\"Threshold: {threshold}\")\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(10, 5))\n",
                "# Identify normal label indices\n",
                "# We need to know which integer labels correspond to 'good'\n",
                "# class_names list has the strings.\n",
                "normal_indices = [i for i, name in enumerate(class_names) if name.endswith('_good')]\n",
                "\n",
                "# Create mask for normal and anomaly\n",
                "is_normal = np.isin(labels, normal_indices)\n",
                "\n",
                "plt.hist(errors[is_normal], bins=20, alpha=0.5, label='Normal')\n",
                "plt.hist(errors[~is_normal], bins=20, alpha=0.5, label='Anomaly')\n",
                "plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
                "plt.legend()\n",
                "plt.title(\"Reconstruction Error Distribution\")\n",
                "plt.show()"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15e236d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Test Data\n",
                "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "    TEST_DIR,\n",
                "    label_mode='int',\n",
                "    image_size=IMG_SIZE,\n",
                "    batch_size=1,\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "def predict_anomaly(model, dataset, threshold=None):\n",
                "    reconstruction_errors = []\n",
                "    labels = []\n",
                "    \n",
                "    for image, label in dataset:\n",
                "        image = preprocess(image)\n",
                "        reconstructed = model.predict(image, verbose=0)\n",
                "        loss = np.mean(np.abs(image - reconstructed))\n",
                "        reconstruction_errors.append(loss)\n",
                "        labels.append(label.numpy()[0])\n",
                "        \n",
                "    return np.array(reconstruction_errors), np.array(labels)\n",
                "\n",
                "errors, labels = predict_anomaly(autoencoder, test_ds)\n",
                "\n",
                "# Determine threshold (e.g., 95th percentile of errors)\n",
                "# Ideally this is done on a validation set of normal images\n",
                "threshold = np.percentile(errors, 90)\n",
                "print(f\"Threshold: {threshold}\")\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.hist(errors[labels==0], bins=20, alpha=0.5, label='Normal')\n",
                "plt.hist(errors[labels!=0], bins=20, alpha=0.5, label='Anomaly')\n",
                "plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
                "plt.legend()\n",
                "plt.title(\"Reconstruction Error Distribution\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Advanced Evaluation for Autoencoder\n",
                "import numpy as np\n",
                "\n",
                "print(\"Evaluating Autoencoder...\")\n",
                "\n",
                "# 1. Calculate Reconstruction Error (MSE) for all test images\n",
                "reconstructions = autoencoder.predict(test_ds)\n",
                "mse_scores = []\n",
                "y_true_labels = []\n",
                "\n",
                "# We need to iterate through test_ds to get original images and labels\n",
                "# Note: test_ds yields (images, labels)\n",
                "idx = 0\n",
                "for images, labels in test_ds:\n",
                "    batch_recon = reconstructions[idx : idx + len(images)]\n",
                "    batch_mse = np.mean(np.square(images - batch_recon), axis=(1, 2, 3))\n",
                "    mse_scores.extend(batch_mse)\n",
                "    y_true_labels.extend(labels.numpy())\n",
                "    idx += len(images)\n",
                "\n",
                "mse_scores = np.array(mse_scores)\n",
                "y_true_labels = np.array(y_true_labels)\n",
                "\n",
                "# 2. Calculate AUC-ROC using MSE as anomaly score\n",
                "# Note: Higher MSE = Anomaly (1), Lower MSE = Normal (0)\n",
                "# Ensure labels are 0 (Normal) and 1 (Anomaly)\n",
                "auc = calculate_auc(y_true_labels, mse_scores)\n",
                "print(f\"AUC-ROC: {auc:.4f}\")\n",
                "\n",
                "# 3. Calculate F1-Score (requires thresholding)\n",
                "# Simple strategy: use mean + 2*std of normal samples from validation set as threshold\n",
                "# For now, we'll just pick a threshold that maximizes F1 on test set for demonstration\n",
                "best_f1 = 0\n",
                "best_thresh = 0\n",
                "thresholds = np.linspace(mse_scores.min(), mse_scores.max(), 100)\n",
                "\n",
                "for thresh in thresholds:\n",
                "    y_pred_bin = (mse_scores > thresh).astype(int)\n",
                "    f1 = calculate_f1(y_true_labels, y_pred_bin)\n",
                "    if f1 > best_f1:\n",
                "        best_f1 = f1\n",
                "        best_thresh = thresh\n",
                "\n",
                "print(f\"Best F1-Score: {best_f1:.4f} (at threshold {best_thresh:.4f})\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Export Model\n",
                "We save the trained model for later use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create models directory if it doesn't exist\n",
                "models_dir = \"../models\"\n",
                "os.makedirs(models_dir, exist_ok=True)\n",
                "\n",
                "# Save the model\n",
                "model_name = \"autoencoder\"\n",
                "category_name = TARGET_CATEGORY if 'TARGET_CATEGORY' in locals() else 'all_categories'\n",
                "save_path = os.path.join(models_dir, f\"{model_name}_{category_name}.keras\")\n",
                "model.save(save_path)\n",
                "print(f\"Model saved to {save_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
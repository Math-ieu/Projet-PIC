{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "14fcffbc",
            "metadata": {},
            "source": [
                "# 04. Convolutional Autoencoder (Unsupervised)\n",
                "\n",
                "## Introduction\n",
                "This notebook implements a Convolutional Autoencoder (CAE) for unsupervised anomaly detection.\n",
                "The model is trained ONLY on normal images to learn to reconstruct them.\n",
                "Anomalies are detected by high reconstruction error.\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfa73889",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, models\n",
                "\n",
                "tf.random.set_seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb76c98b",
            "metadata": {},
            "source": [
                "## 1. Data Loading\n",
                "For the Autoencoder, we train ONLY on the 'train/good' folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "beee0df6",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "from src.preprocessing.dataset import load_and_split_data\n",
                "\n",
                "IMG_SIZE = (256, 256)\n",
                "BATCH_SIZE = 16\n",
                "DATA_DIR = \"../data/raw\"\n",
                "TARGET_CATEGORY = 'bottle' # Train only on this category\n",
                "\n",
                "# Load data\n",
                "print(f\"Loading and splitting data for {TARGET_CATEGORY}...\")\n",
                "train_df, val_df, test_df, class_names = load_and_split_data(DATA_DIR, target_category=TARGET_CATEGORY, augment=True)\n",
                "\n",
                "# Filter for normal samples only for Autoencoder training\n",
                "print(\"Filtering for normal samples...\")\n",
                "normal_train_df = train_df[train_df['label_str'].str.endswith('_good')]\n",
                "print(f\"Normal training samples: {len(normal_train_df)}\")\n",
                "\n",
                "# Dataset creation helper\n",
                "def process_path(filepath):\n",
                "    img = tf.io.read_file(filepath)\n",
                "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
                "    img = tf.image.resize(img, IMG_SIZE)\n",
                "    # Rescale to [0, 1]\n",
                "    img = tf.cast(img, tf.float32) / 255.0\n",
                "    return img\n",
                "\n",
                "def create_dataset(dataframe, batch_size=16, shuffle=False):\n",
                "    filepaths = dataframe['filepath'].values\n",
                "    # Autoencoder doesn't need labels for training, but we need them for evaluation\n",
                "    # For training, we can just return images\n",
                "    ds = tf.data.Dataset.from_tensor_slices(filepaths)\n",
                "    ds = ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "    \n",
                "    if shuffle:\n",
                "        ds = ds.shuffle(buffer_size=1000)\n",
                "    \n",
                "    # Autoencoder expects (x, x)\n",
                "    ds = ds.map(lambda x: (x, x))\n",
                "    \n",
                "    ds = ds.batch(batch_size)\n",
                "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
                "    return ds\n",
                "\n",
                "AUTOTUNE = tf.data.AUTOTUNE\n",
                "train_ds = create_dataset(normal_train_df, BATCH_SIZE, shuffle=True)\n",
                "# We can use val_df (filtered) for validation if we want\n",
                "normal_val_df = val_df[val_df['label_str'].str.endswith('_good')]\n",
                "val_ds = create_dataset(normal_val_df, BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(\"Datasets created.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "931cff80",
            "metadata": {},
            "source": [
                "## 2. Model Architecture\n",
                "Encoder-Decoder architecture."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c95bdcb6",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_autoencoder(input_shape):\n",
                "    # Encoder\n",
                "    inputs = layers.Input(shape=input_shape)\n",
                "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(inputs)\n",
                "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    \n",
                "    # Latent space\n",
                "    shape_before_flattening = tf.keras.backend.int_shape(x)[1:]\n",
                "    x = layers.Flatten()(x)\n",
                "    latent = layers.Dense(128, name='latent_vector')(x)\n",
                "    \n",
                "    # Decoder\n",
                "    x = layers.Dense(np.prod(shape_before_flattening))(latent)\n",
                "    x = layers.Reshape(shape_before_flattening)(x)\n",
                "    \n",
                "    x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
                "    \n",
                "    outputs = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
                "    \n",
                "    model = models.Model(inputs, outputs, name='autoencoder')\n",
                "    return model\n",
                "\n",
                "autoencoder = create_autoencoder(IMG_SIZE + (3,))\n",
                "autoencoder.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b28e0cfc",
            "metadata": {},
            "source": [
                "## 3. Training\n",
                "Loss function is Mean Squared Error (MSE) between input and output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "978ac196",
            "metadata": {},
            "outputs": [],
            "source": [
                "autoencoder.compile(optimizer='adam', loss='mse')\n",
                "\n",
                "history = autoencoder.fit(\n",
                "    train_ds,\n",
                "    epochs=20,\n",
                "    # In unsupervised setting, we often use a split of train set as validation,\n",
                "    # or just monitor loss.\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "17b6c377",
            "metadata": {},
            "source": [
                "# Create Test Dataset (with labels for evaluation)\n",
                "def process_path_label(filepath, label):\n",
                "    img = tf.io.read_file(filepath)\n",
                "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
                "    img = tf.image.resize(img, IMG_SIZE)\n",
                "    img = tf.cast(img, tf.float32) / 255.0\n",
                "    return img, label\n",
                "\n",
                "def create_test_dataset(dataframe):\n",
                "    filepaths = dataframe['filepath'].values\n",
                "    labels = dataframe['label'].values\n",
                "    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
                "    ds = ds.map(process_path_label, num_parallel_calls=AUTOTUNE)\n",
                "    ds = ds.batch(1) # Batch size 1 for individual prediction\n",
                "    return ds\n",
                "\n",
                "test_ds = create_test_dataset(test_df)\n",
                "\n",
                "def predict_anomaly(model, dataset, threshold=None):\n",
                "    reconstruction_errors = []\n",
                "    labels = []\n",
                "    \n",
                "    for image, label in dataset:\n",
                "        reconstructed = model.predict(image, verbose=0)\n",
                "        loss = np.mean(np.abs(image - reconstructed))\n",
                "        reconstruction_errors.append(loss)\n",
                "        labels.append(label.numpy()[0])\n",
                "        \n",
                "    return np.array(reconstruction_errors), np.array(labels)\n",
                "\n",
                "print(\"Predicting anomalies on test set...\")\n",
                "errors, labels = predict_anomaly(autoencoder, test_ds)\n",
                "\n",
                "# Determine threshold (e.g., 90th percentile of errors)\n",
                "threshold = np.percentile(errors, 90)\n",
                "print(f\"Threshold: {threshold}\")\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(10, 5))\n",
                "# Identify normal label indices\n",
                "# We need to know which integer labels correspond to 'good'\n",
                "# class_names list has the strings.\n",
                "normal_indices = [i for i, name in enumerate(class_names) if name.endswith('_good')]\n",
                "\n",
                "# Create mask for normal and anomaly\n",
                "is_normal = np.isin(labels, normal_indices)\n",
                "\n",
                "plt.hist(errors[is_normal], bins=20, alpha=0.5, label='Normal')\n",
                "plt.hist(errors[~is_normal], bins=20, alpha=0.5, label='Anomaly')\n",
                "plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
                "plt.legend()\n",
                "plt.title(\"Reconstruction Error Distribution\")\n",
                "plt.show()"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15e236d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Test Data\n",
                "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "    TEST_DIR,\n",
                "    label_mode='int',\n",
                "    image_size=IMG_SIZE,\n",
                "    batch_size=1,\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "def predict_anomaly(model, dataset, threshold=None):\n",
                "    reconstruction_errors = []\n",
                "    labels = []\n",
                "    \n",
                "    for image, label in dataset:\n",
                "        image = preprocess(image)\n",
                "        reconstructed = model.predict(image, verbose=0)\n",
                "        loss = np.mean(np.abs(image - reconstructed))\n",
                "        reconstruction_errors.append(loss)\n",
                "        labels.append(label.numpy()[0])\n",
                "        \n",
                "    return np.array(reconstruction_errors), np.array(labels)\n",
                "\n",
                "errors, labels = predict_anomaly(autoencoder, test_ds)\n",
                "\n",
                "# Determine threshold (e.g., 95th percentile of errors)\n",
                "# Ideally this is done on a validation set of normal images\n",
                "threshold = np.percentile(errors, 90)\n",
                "print(f\"Threshold: {threshold}\")\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.hist(errors[labels==0], bins=20, alpha=0.5, label='Normal')\n",
                "plt.hist(errors[labels!=0], bins=20, alpha=0.5, label='Anomaly')\n",
                "plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
                "plt.legend()\n",
                "plt.title(\"Reconstruction Error Distribution\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Advanced Evaluation for Autoencoder\n",
                "from src.evaluation.metrics import calculate_auc, calculate_f1\n",
                "import numpy as np\n",
                "\n",
                "print(\"Evaluating Autoencoder...\")\n",
                "\n",
                "# 1. Calculate Reconstruction Error (MSE) for all test images\n",
                "reconstructions = autoencoder.predict(test_ds)\n",
                "mse_scores = []\n",
                "y_true_labels = []\n",
                "\n",
                "# We need to iterate through test_ds to get original images and labels\n",
                "# Note: test_ds yields (images, labels)\n",
                "idx = 0\n",
                "for images, labels in test_ds:\n",
                "    batch_recon = reconstructions[idx : idx + len(images)]\n",
                "    batch_mse = np.mean(np.square(images - batch_recon), axis=(1, 2, 3))\n",
                "    mse_scores.extend(batch_mse)\n",
                "    y_true_labels.extend(labels.numpy())\n",
                "    idx += len(images)\n",
                "\n",
                "mse_scores = np.array(mse_scores)\n",
                "y_true_labels = np.array(y_true_labels)\n",
                "\n",
                "# 2. Calculate AUC-ROC using MSE as anomaly score\n",
                "# Note: Higher MSE = Anomaly (1), Lower MSE = Normal (0)\n",
                "# Ensure labels are 0 (Normal) and 1 (Anomaly)\n",
                "auc = calculate_auc(y_true_labels, mse_scores)\n",
                "print(f\"AUC-ROC: {auc:.4f}\")\n",
                "\n",
                "# 3. Calculate F1-Score (requires thresholding)\n",
                "# Simple strategy: use mean + 2*std of normal samples from validation set as threshold\n",
                "# For now, we'll just pick a threshold that maximizes F1 on test set for demonstration\n",
                "best_f1 = 0\n",
                "best_thresh = 0\n",
                "thresholds = np.linspace(mse_scores.min(), mse_scores.max(), 100)\n",
                "\n",
                "for thresh in thresholds:\n",
                "    y_pred_bin = (mse_scores > thresh).astype(int)\n",
                "    f1 = calculate_f1(y_true_labels, y_pred_bin)\n",
                "    if f1 > best_f1:\n",
                "        best_f1 = f1\n",
                "        best_thresh = thresh\n",
                "\n",
                "print(f\"Best F1-Score: {best_f1:.4f} (at threshold {best_thresh:.4f})\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}